{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To read Dataset\n",
    "import pandas as pd\n",
    "#to shuffle dataset randomly\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import Imputer\n",
    "import pylab as pl\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "# DF TO EXCEL\n",
    "from pandas import ExcelWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from pandas import ExcelWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = pd.read_csv('C:\\\\Users\\\\Admin\\\\Desktop\\\\CS3123\\\\Data_Set\\\\Dataset.csv')\n",
    "#pd.to_numeric(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop index and unnessery featuer \n",
    "Dataset.drop(['0', '0.1','3'], axis=1,inplace=True)\n",
    "D4=Dataset.drop(Dataset.columns.to_series()[\"6\":\"445\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D4.sort_values('4', ascending = 'True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2771, 4)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_Data=D4.drop(D4.index[1319])\n",
    "H_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Gender Featuer is removed, so we need to ckeach how many samples doset have value in that feature\n",
    "#print(H_Data.head(1320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2355, 4)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the subset feuters(AGE(GENDER(HIGHIT(WEGHIT)))) \n",
    "H_Data=D4[np.isfinite(D4['5'])]\n",
    "H_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2354, 4)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the subset feuters(AGE(GENDER(HIGHIT(WEGHIT)))) \n",
    "w_Data=D4[np.isfinite(D4['4'])]\n",
    "w_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>187.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>174.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>157.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2740</th>\n",
       "      <td>65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2741</th>\n",
       "      <td>67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2742</th>\n",
       "      <td>66</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>177.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2754</th>\n",
       "      <td>65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>78</td>\n",
       "      <td>1.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2761</th>\n",
       "      <td>70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>61</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>63</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>183.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2351 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1    2      4      5\n",
       "0     79  1.0   73.0  164.0\n",
       "1     76  0.0   74.0  172.0\n",
       "3     63  1.0   75.0  168.0\n",
       "4     80  0.0   85.0  187.0\n",
       "5     71  0.0   78.0  165.0\n",
       "6     82  0.0   94.0  175.0\n",
       "7     65  1.0   87.0  165.0\n",
       "8     81  0.0   79.0  167.0\n",
       "9     72  0.0   78.0  173.0\n",
       "10    74  1.0   70.0  165.0\n",
       "11    57  0.0   90.0  175.0\n",
       "14    65  0.0   80.0  174.0\n",
       "15    82  0.0   65.0  165.0\n",
       "16    67  0.0   94.0  170.0\n",
       "17    66  0.0   80.0  175.0\n",
       "18    69  1.0   77.0  157.0\n",
       "19    70  0.0  105.0  192.0\n",
       "20    65  1.0   65.0  164.0\n",
       "21    51  0.0   62.0  162.0\n",
       "22    82  1.0   62.0  165.0\n",
       "23    82  0.0   83.0  180.0\n",
       "24    77  0.0   70.0  164.0\n",
       "25    89  0.0   65.0  175.0\n",
       "26    76  0.0   75.0  182.0\n",
       "27    70  1.0   75.0  160.0\n",
       "28    76  0.0   80.0  178.0\n",
       "29    67  1.0   86.0  164.0\n",
       "30    78  0.0  170.0   63.0\n",
       "31    78  0.0   90.0  190.0\n",
       "32    62  1.0   87.0  168.0\n",
       "...   ..  ...    ...    ...\n",
       "2740  65  0.0  184.0   82.0\n",
       "2741  67  0.0   90.0  165.0\n",
       "2742  66  1.0   80.0  168.0\n",
       "2743  75  0.0  102.0  176.0\n",
       "2744  64  1.0   65.0  156.0\n",
       "2745  60  1.0   73.0  160.0\n",
       "2746  70  0.0   80.0  176.0\n",
       "2747  69  1.0   95.0  164.0\n",
       "2748  62  0.0   76.0  177.0\n",
       "2749  77  0.0   80.0  180.0\n",
       "2750  62  0.0   90.0  183.0\n",
       "2751  63  0.0   70.0  168.0\n",
       "2752  73  0.0  100.0  170.0\n",
       "2753  65  1.0  160.0   64.0\n",
       "2754  65  1.0  170.0   65.0\n",
       "2755  78  1.0  163.0   72.0\n",
       "2756  73  1.0  165.0   69.0\n",
       "2758  67  1.0  170.0   65.0\n",
       "2759  75  0.0  182.0   87.0\n",
       "2761  70  1.0  165.0   65.0\n",
       "2762  75  1.0   68.0  156.0\n",
       "2763  61  1.0   67.0  168.0\n",
       "2764  57  0.0  105.0  188.0\n",
       "2765  75  1.0   50.0  163.0\n",
       "2766  68  0.0  103.0  178.0\n",
       "2767  63  1.0   79.0  158.0\n",
       "2768  64  0.0   78.0  182.0\n",
       "2769  68  0.0   84.0  165.0\n",
       "2770  52  0.0   82.0  168.0\n",
       "2771  68  0.0   90.0  183.0\n",
       "\n",
       "[2351 rows x 4 columns]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing Missing values and Keeping Gender feature\n",
    "w_Data.dropna(axis='rows', how='any')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing outliers from all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Removing outliers from age "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "D4 = w_Data[(w_Data['1']> 0)]\n",
    "w_Data = w_Data[(w_Data['1']<100)]\n",
    "\n",
    "#w_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_Data.sort_values(\"1\", ascending = 'True')\n",
    "A_Data=w_Data\n",
    "#A_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A_Data = A_Data[(A_Data['5']> 150)]\n",
    "A_Data = A_Data[(A_Data['5']<190)]\n",
    "A_Data.sort_values(\"5\", ascending = 'True')\n",
    "AH_Data=A_Data\n",
    "#AH_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Removing outliers from Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AH_Data = AH_Data[(AH_Data['4']> 38)]\n",
    "AH_Data = AH_Data[(AH_Data['4']<108)]\n",
    "AH_Data.sort_values(\"4\", ascending = 'True')\n",
    "AHW_Data=AH_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Female subset\n",
    "F_Data=AHW_Data[AHW_Data['2'] == 0.0]\n",
    "#Male subset\n",
    "M_Data=AHW_Data[AHW_Data['2'] == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1321, 4)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(878, 4)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1    2      4      5\n",
      "1     76  0.0   74.0  172.0\n",
      "4     80  0.0   85.0  187.0\n",
      "5     71  0.0   78.0  165.0\n",
      "6     82  0.0   94.0  175.0\n",
      "8     81  0.0   79.0  167.0\n",
      "9     72  0.0   78.0  173.0\n",
      "11    57  0.0   90.0  175.0\n",
      "14    65  0.0   80.0  174.0\n",
      "15    82  0.0   65.0  165.0\n",
      "16    67  0.0   94.0  170.0\n",
      "17    66  0.0   80.0  175.0\n",
      "21    51  0.0   62.0  162.0\n",
      "23    82  0.0   83.0  180.0\n",
      "24    77  0.0   70.0  164.0\n",
      "25    89  0.0   65.0  175.0\n",
      "26    76  0.0   75.0  182.0\n",
      "28    76  0.0   80.0  178.0\n",
      "36    61  0.0   80.0  178.0\n",
      "37    58  0.0   92.0  175.0\n",
      "39    68  0.0   75.0  178.0\n",
      "40    76  0.0   94.0  172.0\n",
      "42    74  0.0   75.0  169.0\n",
      "45    61  0.0   81.0  175.0\n",
      "46    70  0.0   72.0  172.0\n",
      "47    60  0.0   74.0  174.0\n",
      "49    71  0.0   74.0  172.0\n",
      "50    66  0.0   96.0  187.0\n",
      "54    65  0.0   85.0  172.0\n",
      "55    76  0.0   72.0  172.0\n",
      "56    80  0.0   85.0  175.0\n",
      "...   ..  ...    ...    ...\n",
      "2713  55  0.0   70.0  172.0\n",
      "2717  71  0.0   74.0  182.0\n",
      "2718  64  0.0   62.0  170.0\n",
      "2721  69  0.0   92.0  173.0\n",
      "2722  70  0.0   72.0  188.0\n",
      "2723  63  0.0   67.0  170.0\n",
      "2724  75  0.0   88.0  180.0\n",
      "2725  65  0.0   84.0  168.0\n",
      "2726  77  0.0   74.0  172.0\n",
      "2727  76  0.0   75.0  178.0\n",
      "2730  69  0.0   80.0  162.0\n",
      "2733  79  0.0   86.0  180.0\n",
      "2734  76  0.0   70.0  171.0\n",
      "2737  73  0.0   74.0  170.0\n",
      "2738  61  0.0   80.0  183.0\n",
      "2739  70  0.0   90.0  174.0\n",
      "2741  67  0.0   90.0  165.0\n",
      "2743  75  0.0  102.0  176.0\n",
      "2746  70  0.0   80.0  176.0\n",
      "2748  62  0.0   76.0  177.0\n",
      "2749  77  0.0   80.0  180.0\n",
      "2750  62  0.0   90.0  183.0\n",
      "2751  63  0.0   70.0  168.0\n",
      "2752  73  0.0  100.0  170.0\n",
      "2764  57  0.0  105.0  188.0\n",
      "2766  68  0.0  103.0  178.0\n",
      "2768  64  0.0   78.0  182.0\n",
      "2769  68  0.0   84.0  165.0\n",
      "2770  52  0.0   82.0  168.0\n",
      "2771  68  0.0   90.0  183.0\n",
      "\n",
      "[1321 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "F_Data.sort_values(\"1\", ascending = 'True')\n",
    "print(F_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M_Data.sort_values(\"1\", ascending = 'True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance Gender subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 442 from female subset to blance data\n",
    "#F_Data.sample(878)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "BF_Data=F_Data.sample(878, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(878, 4)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BF_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [M_Data, BF_Data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_Data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1756, 4)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MF_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MF_Data= MF_Data[(MF_Data['1']> 0)]\n",
    "MF_Data =MF_Data[(MF_Data['1']<100)]\n",
    "\n",
    "#w_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_Data=MF_Data.sort_values(\"1\", ascending = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MF_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extr Female data\n",
    "x_extra_FData=F_Data[:]\n",
    "x_extra_FData=F_Data.drop(['2'],axis=1)\n",
    "y_extra_FData=F_Data['2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraxtinggender lable from Male_Female_Data (MF_Data)\n",
    "y = MF_Data['2']\n",
    "TT=MF_Data.drop(['2'],axis=1)\n",
    "X=TT[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1755, 3)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data  Training 75%/ Testin sets 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The avrege a ccuracy of Logistic regression classifier on test set 0.7965831435079727\n",
      " The avrege a ccuracy of Decision Tree classifier on test set 0.7145785876993166\n",
      " The avrege a ccuracy of KNN classifier on test set 0.7774487471526196\n",
      " The avrege a ccuracy of LDA classifier on test set 0.7970387243735763\n"
     ]
    }
   ],
   "source": [
    "#Create Training and Test Sets and Apply Scaling\n",
    "Avg_scores_LR = []\n",
    "Avg_scores_DT = []\n",
    "Avg_scores_KNN= []\n",
    "Avg_scores_LDA= []\n",
    "\n",
    "\n",
    "for x in range(10):\n",
    "    seed= 1+x\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=seed)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "#Fiting data to ML Models\n",
    "    #Logistic Regression\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "    scores_LR = logreg.score(X_test, y_test)\n",
    "    Avg_scores_LR.append(scores_LR.mean())\n",
    "    \n",
    "    \n",
    "    #Decision Tree\n",
    "    DT = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "    scores_DT = DT.score(X_test, y_test)\n",
    "    Avg_scores_DT.append(scores_DT.mean())\n",
    "    \n",
    "    #K-Nearest Neighbors\n",
    "    KNN = KNeighborsClassifier()\n",
    "    KNN.fit(X_train, y_train)\n",
    "    scores_k= KNN.score(X_test, y_test)\n",
    "    Avg_scores_KNN.append(scores_k.mean())\n",
    "    \n",
    "    #Linear Discriminant Analysis\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "    scores_lda= lda.score(X_test, y_test)\n",
    "    Avg_scores_LDA.append(scores_lda.mean())\n",
    "    \n",
    "    \n",
    "#print(Avg_scores)\n",
    "ALR=sum(Avg_scores_LR) / len(Avg_scores_LR)\n",
    "print(' The avrege a ccuracy of Logistic regression classifier on test set',ALR)\n",
    "      \n",
    "ADT=sum(Avg_scores_DT) / len(Avg_scores_DT)\n",
    "print(' The avrege a ccuracy of Decision Tree classifier on test set',ADT)\n",
    "      \n",
    "AKNN=sum(Avg_scores_KNN) / len(Avg_scores_KNN)\n",
    "print(' The avrege a ccuracy of KNN classifier on test set',AKNN)\n",
    "\n",
    "LDA=sum(Avg_scores_LDA) / len(Avg_scores_LDA)\n",
    "print(' The avrege a ccuracy of LDA classifier on test set',LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(random_state= 0)\n",
    "logit.fit(X_train, y_train)\n",
    "y_predicted = logit.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *************************END TASK 6*****************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['0','1']\n",
    "plt.title('Confusion Matrix - Test Data')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Prediction')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for the testing set\n",
    "y_pred_class = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7949886104783599\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    223\n",
       "0.0    216\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the class distribution of the testing set (using a Pandas Series method)\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[167  49]\n",
      " [ 41 182]]\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: first argument is true values, second argument is predicted values\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save confusion matrix and slice into four pieces\n",
    "confusion = metrics.confusion_matrix(y_test, y_predicted)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics computed from a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Classification Accuracy:** Overall, how often is the classifier correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7949886104783599\n",
      "0.7949886104783599\n"
     ]
    }
   ],
   "source": [
    "print((TP + TN) / float(TP + TN + FP + FN))\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Classification Error:** Overall, how often is the classifier incorrect?\n",
    "\n",
    "- Also known as \"Misclassification Rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20501138952164008\n",
      "0.2050113895216401\n"
     ]
    }
   ],
   "source": [
    "print((FP + FN) / float(TP + TN + FP + FN))\n",
    "print(1 - metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Sensitivity:** When the actual value is positive, how often is the prediction correct?\n",
    "\n",
    "- How \"sensitive\" is the classifier to detecting positive instances?\n",
    "- Also known as \"True Positive Rate\" or \"Recall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8161434977578476\n",
      "0.8161434977578476\n"
     ]
    }
   ],
   "source": [
    "print(TP / float(TP + FN))\n",
    "print(metrics.recall_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Specificity:** When the actual value is negative, how often is the prediction correct?\n",
    "\n",
    "- How \"specific\" (or \"selective\") is the classifier in predicting positive instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7731481481481481\n"
     ]
    }
   ],
   "source": [
    "print(TN / float(TN + FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False Positive Rate:** When the actual value is negative, how often is the prediction incorrect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22685185185185186\n"
     ]
    }
   ],
   "source": [
    "print(FP / float(TN + FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision:** When a positive value is predicted, how often is the prediction correct?\n",
    "\n",
    "- How \"precise\" is the classifier when predicting positive instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7878787878787878\n",
      "0.7878787878787878\n"
     ]
    }
   ],
   "source": [
    "print(TP / float(TP + FP))\n",
    "print(metrics.precision_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[167  49]\n",
      " [ 41 182]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.77      0.79       216\n",
      "        1.0       0.79      0.82      0.80       223\n",
      "\n",
      "avg / total       0.80      0.79      0.79       439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred_y = logreg.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred_y))\n",
    "print(classification_report(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
